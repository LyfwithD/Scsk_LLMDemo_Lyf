import streamlit as st
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_core.messages import HumanMessage
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate

def save_file(uploadedfile):
    with open(os.path.join("tempDir", uploadedfile.name), "wb") as f:
        f.write(uploadedfile.getbuffer())

def generate_response(prompt, model="qwen-plus"):
    chatLLM = ChatTongyi(model_name=model, streaming=True)
    resp = chatLLM.stream([HumanMessage(content=prompt)], streaming=True)
    for r in resp:
        yield r.content

with st.sidebar:
    st.title('The contents below are generated by ali-qwen.')
    "[API Document of qwen](https://help.aliyun.com/zh/dashscope/developer-reference/api-details?spm=a2c4g.11186623.0.i48)"

st.title("üìù File Q&A with QWen")
uploaded_file = st.file_uploader("Upload an article", type=["pdf"])

question = st.text_input(
    "Ask something about the article",
    placeholder="What does XXX mean in this article?",
    disabled=not uploaded_file,
)

if uploaded_file and question:
    if uploaded_file is not None:
        save_file(uploaded_file)
    loader = PyPDFLoader("tempDir/"+uploaded_file.name)
    pages = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=400,
        chunk_overlap=100,
        length_function=len,
        add_start_index=True
    )
    paragraphs = []
    for page in pages:
        paragraphs.extend(text_splitter.create_documents([page.page_content]))
    db = FAISS.from_documents(paragraphs, DashScopeEmbeddings())

    chatLLM = ChatTongyi(model_name="qwen-plus", streaming=True)
    retriever = db.as_retriever()
    docs = retriever.get_relevant_documents(question)

    prompt_template = """
            You are a Q&A chatbot. Your task is to answer user questions based on the given known information described below. 
            Make sure that your response is based solely on what you already know below. 
            Don't make up the answer.
            If the following information is not sufficient to answer the user's question, reply "I can't answer your question." directly.

            Given information:
            {info}

            User's questionÔºö
            {question}
            """

    template = PromptTemplate.from_template(prompt_template)
    chain = template | chatLLM
    info = ""
    for doc in docs:
        info += doc.page_content

    ## ÈùûÊµÅÂºèËæìÂá∫
    # st.write(chain.invoke({"info": info, "question": question}).content)

    ## ÊµÅÂºèËæìÂá∫
    st.write_stream(chain.stream({"info": info, "question": question}))
